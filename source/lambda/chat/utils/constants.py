#!/usr/bin/env python
# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

import os

from utils.enum_types import BedrockModelProviders, LLMProviderTypes

# Chat environment variables
USE_CASE_CONFIG_TABLE_NAME_ENV_VAR = "USE_CASE_CONFIG_TABLE_NAME"
USE_CASE_CONFIG_RECORD_KEY_ENV_VAR = "USE_CASE_CONFIG_RECORD_KEY"
CONVERSATION_TABLE_NAME_ENV_VAR = "CONVERSATION_TABLE_NAME"
KENDRA_INDEX_ID_ENV_VAR = "KENDRA_INDEX_ID"
BEDROCK_KNOWLEDGE_BASE_ID_ENV_VAR = "BEDROCK_KNOWLEDGE_BASE_ID"
WEBSOCKET_CALLBACK_URL_ENV_VAR = "WEBSOCKET_CALLBACK_URL"
DDB_MESSAGE_TTL_ENV_VAR = "DDB_MESSAGE_TTL"
USE_CASE_UUID_ENV_VAR = "USE_CASE_UUID"
TRACE_ID_ENV_VAR = "_X_AMZN_TRACE_ID"
MODEL_INFO_TABLE_NAME_ENV_VAR = "MODEL_INFO_TABLE_NAME"
CHAT_REQUIRED_ENV_VARS = [
    USE_CASE_CONFIG_TABLE_NAME_ENV_VAR,
    USE_CASE_CONFIG_RECORD_KEY_ENV_VAR,
    CONVERSATION_TABLE_NAME_ENV_VAR,
    WEBSOCKET_CALLBACK_URL_ENV_VAR,
    TRACE_ID_ENV_VAR,
    MODEL_INFO_TABLE_NAME_ENV_VAR,
]
LLM_CONFIG_RECORD_FIELD_NAME = "key"
RAG_CHAT_IDENTIFIER = "RAGChat"
CHAT_IDENTIFIER = "Chat"
TEMPERATURE_PLACEHOLDER = "<<temperature>>"
TEMPERATURE_PLACEHOLDER_STR = "temperature"
CLIENT_ID_ENV_VAR = "CLIENT_ID"
USER_POOL_ID_ENV_VAR = "USER_POOL_ID"

# Event keys
USER_ID_EVENT_KEY = "UserId"
CONVERSATION_ID_EVENT_KEY = "conversationId"
MESSAGE_ID_EVENT_KEY = "messageId"
QUESTION_EVENT_KEY = "question"
PROMPT_EVENT_KEY = "promptTemplate"
AUTH_TOKEN_EVENT_KEY = "authToken"
REQUEST_CONTEXT_KEY = "requestContext"
MESSAGE_KEY = "message"

# Chat constants across models
END_CONVERSATION_TOKEN = "##END_CONVERSATION##"
METRICS_SERVICE_NAME = f"GAABUseCase-{os.getenv(USE_CASE_UUID_ENV_VAR)}"
DEFAULT_DDB_MESSAGE_TTL = 60 * 60 * 24  # 24 hours in seconds
DEFAULT_RAG_CHAIN_TYPE = "stuff"
DEFAULT_KENDRA_NUMBER_OF_DOCS = 2
DEFAULT_BEDROCK_KNOWLEDGE_BASE_NUMBER_OF_DOCS = 2
DEFAULT_RETURN_SOURCE_DOCS_MODE = False
DEFAULT_REPHRASE_QUESTION_MODE = False
DEFAULT_SCORE_THRESHOLD = 0.0
DEFAULT_MAX_TOKENS_TO_SAMPLE = 256
DEFAULT_VERBOSE_MODE = False
DEFAULT_DISAMBIGUATION_ENABLED_MODE = True
DEFAULT_RAG_ENABLED_MODE = False
DEFAULT_DISAMBIGUATION_ENABLED_MODE = True
DEFAULT_STREAMING_MODE = False
DISAMBIGUATION_PROMPT_PLACEHOLDERS = ["input", "history"]
DEFAULT_RETURN_GENERATED_RAG_QUESTION = True
DEFAULT_REPHRASE_RAG_QUESTION = True
SOURCE_DOCUMENTS_RECEIVED_KEY = "context"
SOURCE_DOCUMENTS_OUTPUT_KEY = "source_documents"
LLM_RESPONSE_KEY = "answer"
DEFAULT_SAGEMAKER_MODEL_ID = "default"
HISTORY_KEY = "history"
INPUT_KEY = "input"
OUTPUT_KEY = "answer"
CONTEXT_KEY = "context"
HUMAN_PREFIX = "human_prefix"
AI_PREFIX = "ai_prefix"
SYSTEM_PREFIX = "system"
USER_ID_KEY = "user_id"
CONVERSATION_ID_KEY = "conversation_id"
MESSAGE_ID_KEY = "message_id"
RAG_CONVERSATION_TRACER_KEY = "retrievalAugmentedConversationInvocation"
CONVERSATION_TRACER_KEY = "conversationInvocation"
PAYLOAD_DATA_KEY = "data"
PAYLOAD_SOURCE_DOCUMENT_KEY = "sourceDocument"
REPHRASED_QUERY_KEY = "rephrased_query"

SAGEMAKER_ENDPOINT_ARGS = [
    "CustomAttributes",
    "TargetModel",
    "TargetVariant",
    "TargetContainerHostname",
    "InferenceId",
    "EnableExplanations",
    "InferenceComponentName",
]

BEDROCK_GUARDRAILS_KEY = "guardrail_config"
BEDROCK_GUARDRAIL_IDENTIFIER_KEY = "GuardrailIdentifier"
BEDROCK_GUARDRAIL_VERSION_KEY = "GuardrailVersion"
BEDROCK_INFERENCE_PROFILE_MODEL = "inference-profile"

# Top-level Bedrock Converse API parameters
# key is the Bedrock API param name, and value is the Langchain property name
TOP_LEVEL_PARAMS_MAPPING = {
    "temperature": "temperature",
    "topP": "top_p",
    "maxTokens": "max_tokens",
    "stopSequences": "stop_sequences",
}

DEFAULT_RAG_RBAC_ENABLED_STATUS = False
